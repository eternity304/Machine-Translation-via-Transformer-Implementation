{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyOoDwIcuvg8uAFAWW396i7r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":117,"metadata":{"id":"vbLviupLkKQR","executionInfo":{"status":"ok","timestamp":1705273907694,"user_tz":300,"elapsed":2350,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d54dd72-a07b-45c2-d1d7-d40189e136d8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math, copy, time\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import seaborn\n","seaborn.set_context(context=\"talk\")\n","%matplotlib inline\n","\n","import jieba\n","from nltk import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from collections import Counter\n","import torchtext\n","glove = torchtext.vocab.GloVe(name=\"6B\", # Specified corpus used to train embedding\n","                              dim=50)    # Specified dimension of embedding"]},{"cell_type":"markdown","source":["# Embedding Sections"],"metadata":{"id":"kyktZd1PUoM4"}},{"cell_type":"markdown","source":["## Embedding Class\n","-  takes in a tensor, desired output dimension, and number of words in the output space, and blow it up to such"],"metadata":{"id":"wX1hgaH8Hfxv"}},{"cell_type":"code","source":["class  Embeddings(nn.Module):\n","    def __init__(self, dim_model, vocab_size):\n","        '''\n","        dim_model, dimension of  word embedding\n","        vocab_size, number of  entries in the vocab\n","        '''\n","        super(Embeddings, self).__init__()\n","\n","        self.lut = nn.Embedding(vocab_size, dim_model)\n","        self.dim_model = dim_model\n","\n","    def forward(self, x):\n","        '''\n","        regularize  the input by the square root of dim model\n","        '''\n","        return self.lut(x) * math.sqrt(self.dim_model)"],"metadata":{"id":"tVBqOKH7HyRK","executionInfo":{"status":"ok","timestamp":1705254958935,"user_tz":300,"elapsed":605,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","print(\"Embedded Shape:\", embedded.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8m3WZe5hGcaF","executionInfo":{"status":"ok","timestamp":1705254959231,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"9a71d746-052f-48ee-fc0c-a72281527694"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedded Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Positional Embedding\n","- Odd number Indices: PE = sin(pos/10000^(2i/dim_model))\n","- EVen number Indices: PE = cos(pos/10000^(2i/dmodel))"],"metadata":{"id":"VGZiUzXUFSDY"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def  __init__(self, dim_model, p_dropout, max_len=5000):\n","        '''\n","        dim_model, the dimension of embedding\n","        dropout,  the probability of  dropping out\n","        max_len, the longest possible sequence length\n","        '''\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.dropout = nn.Dropout(p_dropout)\n","\n","        # Initiallize Positional embedding\n","        pe = torch.zeros(max_len, dim_model)\n","\n","        # Initialize tensor for each position\n","        # each entry will correspond to index at that position\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","\n","        # For each seqence we have index tensor \"position\" that\n","        # ranges from [0, max_len] and is of shape max_len by 1\n","        # We want to generate a index array for the embeddings\n","        # in the model which means we require an array of size\n","        # max_len by dim_model. We obtain this array by multipling\n","        # the position array by div_term matrix of size 1 by dim_model\n","\n","        # In addition to changing the dimension, the div_term matrix should also\n","        # minimize the entries of the matrix which would help the gradient descent to\n","        # converge in later training\n","\n","        # We implement the above by a regularization using exponention and logarithm\n","        # Notice how we initialize an div_term of half of dim_model which does not make\n","        # sense for matrix multiplication, this is done this such that we are able to compute\n","        # the position term of odd indices and even indivces separately\n","\n","        # If dim_model = 512, div_term is of size 1 by 256, then position * div_term\n","        # would be 512 by 1 * 1 by 256 which would yield a tensor of 512 by 256 which is\n","        # sufficient for representing either the odd or the even indices. We can view the following\n","        # operation as initializing the positional embedding twice, first on the sin with odd indices and\n","        # then on the coisne with even indices.\n","        # Then the following would yield a poistional embedding of size torch.Size([1, 60, 512])\n","        # Lastly, we register the posistion embeddiong as buffer. A buffer can be thought of as an important\n","        # component of the model that does not require gradient but should be recorded and saved\n","\n","        div_term = torch.exp(torch.arange(0, dim_model,  2) * -(math.log(10000.0) / dim_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x):\n","        '''\n","        Take in a x of batch_size by max_sequence_len by embedding_size\n","        even though the max_sequence length is 5000, it is such a big number\n","        and sequence of that length is imporbable. In this case, we simply replace the\n","        2nd dimension of input x with x.size(1) and since positional embedding does not requires\n","        gradeient, we set it to false.\n","        '''\n","        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n","        return self.dropout(x)"],"metadata":{"id":"SnQIhdu5H2Ry","executionInfo":{"status":"ok","timestamp":1705257711558,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["x =  Variable(torch.LongTensor([[100, 2, 421, 508],\n","                                [491, 998, 1, 221]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","output = pe(x)\n","\n","print(\"Positional Result Size:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSgp_uj8OlbX","executionInfo":{"status":"ok","timestamp":1705258657701,"user_tz":300,"elapsed":370,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"ec32035f-3211-4584-ae7b-5b6f0994e31a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Result Size: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["# Encoder Section"],"metadata":{"id":"ewGPPMM1Vjkx"}},{"cell_type":"markdown","source":["## Masking\n","- used in calculation of attention. When generating attention, it is possible to generate future values for the decoder. This is not useful for the training and we want to mask them"],"metadata":{"id":"VdFcuDXYWG23"}},{"cell_type":"code","source":["def subsequent_mask(size):\n","    # We form an upper traingular matrix for masking matrix\n","    # with dimension from the input size. We change the type to uint8\n","    # for efficient use of memory\n","    subsequent_mask = np.triu(np.ones((1, size, size)), k = 1).astype(\"uint8\")\n","    # we convert the numpy to lower triangular and change to torch.tensor\n","    return torch.from_numpy(1 - subsequent_mask)"],"metadata":{"id":"x3jZtqbvSLpl","executionInfo":{"status":"ok","timestamp":1705259159951,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["output = subsequent_mask(5)\n","print(\"Subsequent mask:\", output)\n","plt.figure(figsize=(5,5))\n","plt.imshow(output[0])\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":553},"id":"auD5PHFXXcoh","executionInfo":{"status":"ok","timestamp":1705259279318,"user_tz":300,"elapsed":3,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"96fdb305-4ad1-48d4-a866-fd49aacb5df7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Subsequent mask: tensor([[[1, 0, 0, 0, 0],\n","         [1, 1, 0, 0, 0],\n","         [1, 1, 1, 0, 0],\n","         [1, 1, 1, 1, 0],\n","         [1, 1, 1, 1, 1]]], dtype=torch.uint8)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 500x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Attention QKV Calculation\n","- attention gives salience to porition of the input\n","- The weight derived from Q and K will be done by dot-product attention and additive attention"],"metadata":{"id":"-095C2UZZH_V"}},{"cell_type":"code","source":["def attention(query, key, value, mask=None, dropout=None):\n","\n","    dim_k = query.size(-1) # The last dim of query, usually the dimension of word embedding\n","    # Using the formula for attention, we compute the scores\n","    # here the the last two dimenion of the key  is switched such that the dimension matches up\n","    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_k)\n","\n","    # For masking operation, if required, we use the mask_fill method for tensor and\n","    # replaces all the 0 with -1e9\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","\n","    # Apply softmax to the attention score, where dim = -1 means a scalar\n","    p_attention = F.softmax(scores, dim = -1)\n","\n","    if dropout:\n","        p_attention = dropout(p_attention)\n","\n","    return torch.matmul(p_attention, value), p_attention"],"metadata":{"id":"88JORQPDXy4l","executionInfo":{"status":"ok","timestamp":1705263063760,"user_tz":300,"elapsed":250,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","pe_output = pe(x)\n","print(\"Positional Embedding shape:\", pe_output.shape)\n","\n","query = key =value = pe_output\n","\n","mask = Variable(torch.zeros(2, 4, 4))\n","attn, p_attn = attention(query, key, value, mask=mask)\n","\n","print(\"Attention Shape:\", attn.shape)\n","print(\"P attention Shape:\", p_attn.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSff8WUXc4AE","executionInfo":{"status":"ok","timestamp":1705263065045,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"6e9a3080-8579-4894-a694-2405515cd337"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Embedding shape: torch.Size([2, 4, 512])\n","Attention Shape: torch.Size([2, 4, 512])\n","P attention Shape: torch.Size([2, 4, 4])\n"]}]},{"cell_type":"markdown","source":["## Multihead Self-Attention\n","- Compute representation by dividing embedding space into different subspaces to process\n"],"metadata":{"id":"aE1PevBMe0eb"}},{"cell_type":"code","source":["def clones(module, n):\n","    '''\n","    In multihead attention, multiple linear layer that are identical are used\n","    they will be initialized by the clones function\n","    module is the layer we want to copy, and N is the number of copy\n","    '''\n","    # We use a for loop to copy from module N times\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n","\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, head, embedding_dim, dropout=0.1):\n","        '''\n","        head as in the number of head\n","        embedding_dim is the dimension of embedding\n","        dropout, is the rate for dropout\n","        '''\n","        super(MultiHeadedAttention, self).__init__()\n","\n","        # Use the assert function to check if embedding_dim can be\n","        # divided by the number of heads\n","        assert embedding_dim % head == 0\n","        self.dim_k = embedding_dim // head\n","        self.head = head\n","        # Here we requires four clones for since Q, K, V each requires 1\n","        # and the concatenation also requires one\n","        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n","        self.attention = None\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, query, key, value, mask=None):\n","        if mask is not None:\n","            mask = mask.unsqueeze(0)\n","\n","        # Number of sequence\n","        batch_size = query.size(0)\n","\n","        # We apply the zip function on linear layers and QKV such that we can\n","        # iterate through each linear layer as well as QKV\n","        # When at each iteration, we pass, the matrix into the linear model\n","        # and we use the view method to reformat its output adding an dimension\n","        # for number of heads which allows each head to receive a part of the input sequence\n","        # -1  means a dimension that fits with the rest\n","\n","        query, key, value = \\\n","            [model(x).view(batch_size, -1, self.head, self.dim_k).transpose(1,2)\n","              for model, x in zip(self.linears, (query, key, value))]\n","\n","        # After obtaining the input from each head, we pass it to the attention\n","        x, self.attention = attention(query, key, value, mask=mask, dropout=self.dropout)\n","\n","        # We reverse the reformat we done in previous steps using the .contiguous() method\n","        # which then allows us to reformat by .view()\n","        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.dim_k)\n","\n","        # Apply the final linear layer and output the result of multiheaded attention\n","        return self.linears[-1](x)"],"metadata":{"id":"eT8uTMjnecJN","executionInfo":{"status":"ok","timestamp":1705263182222,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","pe_output = pe(x)\n","print(\"Positional Embedding shape:\", pe_output.shape)\n","\n","query = key =value = pe_output\n","\n","attn, p_attn = attention(query, key, value)\n","mask = Variable(torch.zeros(8, 4, 4))\n","\n","mha = MultiHeadedAttention(8, 512, 0.2)\n","mha_output = mha(query, key, value, mask)\n","print(\"Multi Headed Attention Output:\", mha_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcZXCz4ClsAO","executionInfo":{"status":"ok","timestamp":1705263182222,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"e76d260e-7fff-4fa8-d969-a73d30cddc62"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Embedding shape: torch.Size([2, 4, 512])\n","Multi Headed Attention Output: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Feed Forward Layer"],"metadata":{"id":"OS8NCg7pnACN"}},{"cell_type":"code","source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, dim_model, dim_ff, dropout=0.1):\n","        '''\n","        dim_model, input dim\n","        dim_ff, output dim, usually we want the same dimension\n","        '''\n","        super(PositionwiseFeedForward, self).__init__()\n","\n","        self.w1 = nn.Linear(dim_model, dim_ff)\n","        self.w2 = nn.Linear(dim_ff, dim_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w2(self.dropout(F.relu(self.w1(x))))"],"metadata":{"id":"Wo2j9mpZmJQl","executionInfo":{"status":"ok","timestamp":1705263713087,"user_tz":300,"elapsed":1,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","pe_output = pe(x)\n","print(\"Positional Embedding shape:\", pe_output.shape)\n","\n","query = key =value = pe_output\n","\n","attn, p_attn = attention(query, key, value)\n","mask = Variable(torch.zeros(8, 4, 4))\n","\n","mha = MultiHeadedAttention(8, 512, 0.2)\n","mha_output = mha(query, key, value, mask)\n","\n","ff = PositionwiseFeedForward(512, 64, 0.2)\n","ff_output = ff(x)\n","print(\"Feed Forward Output Size:\", ff_output.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9D3eEsbQoP0Z","executionInfo":{"status":"ok","timestamp":1705263716286,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"81de6fcf-f850-48fc-803b-b166573c8b18"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Embedding shape: torch.Size([2, 4, 512])\n","Feed Forward Output Size: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Normalization Layer"],"metadata":{"id":"0xza1uhno5qi"}},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, features, eps=1e-6):\n","        '''\n","        features, dimension of embedding\n","        eps, a very small number used in normalization to prevent denominator from being 0\n","        '''\n","        super(LayerNorm, self).__init__()\n","\n","        # Record the parameter for moving average and for later calculatrion\n","        # we record thes parameter into a2 and b2 for regularization\n","        self.a2 = nn.Parameter(torch.ones(features))\n","        self.b2 = nn.Parameter(torch.zeros(features))\n","\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a2*(x-mean)/(std+self.eps) + self.b2"],"metadata":{"id":"gjQvGIt4oYCp","executionInfo":{"status":"ok","timestamp":1705264561165,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","pe_output = pe(x)\n","print(\"Positional Embedding shape:\", pe_output.shape)\n","\n","query = key =value = pe_output\n","\n","attn, p_attn = attention(query, key, value)\n","mask = Variable(torch.zeros(8, 4, 4))\n","\n","mha = MultiHeadedAttention(8, 512, 0.2)\n","mha_output = mha(query, key, value, mask)\n","\n","ff = PositionwiseFeedForward(512, 64, 0.2)\n","ff_output = ff(x)\n","\n","x = ff_output\n","\n","ln = LayerNorm(512, 1e-6)\n","ln_output = ln(x)\n","print(\"Norm Layer Output Shape:\", ln_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlDZJzX0rlPi","executionInfo":{"status":"ok","timestamp":1705264561165,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"a45b98ce-93bc-4883-ddee-c4460e501533"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Embedding shape: torch.Size([2, 4, 512])\n","Norm Layer Output Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Residual Connection\n","- access to prior gradients"],"metadata":{"id":"oBpOmQidsfrL"}},{"cell_type":"code","source":["class ResidualConnection(nn.Module):\n","    def __init__(self, size, dropout=0.1):\n","        '''\n","        size, dim of embedding\n","        dropout, rate of dropout\n","        '''\n","        super(ResidualConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x , reslayer):\n","        return x + self.dropout(reslayer(self.norm(x)))"],"metadata":{"id":"WnodeFIUsCjV","executionInfo":{"status":"ok","timestamp":1705264902876,"user_tz":300,"elapsed":240,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["x = Variable(torch.LongTensor([[30, 90, 88, 27],\n","                               [403, 254, 337, 20]]))\n","emb = Embeddings(512, 1000)\n","embedded = emb(x)\n","\n","x = embedded\n","pe = PositionalEncoding(512, 0.1, 60)\n","pe_output = pe(x)\n","print(\"Positional Embedding shape:\", pe_output.shape)\n","\n","query = key =value = pe_output\n","\n","attn, p_attn = attention(query, key, value)\n","mask = Variable(torch.zeros(8, 4, 4))\n","\n","mha = MultiHeadedAttention(8, 512, 0.2)\n","mha_output = mha(query, key, value, mask)\n","\n","ff = PositionwiseFeedForward(512, 64, 0.2)\n","ff_output = ff(x)\n","\n","x = ff_output\n","\n","ln = LayerNorm(512, 1e-6)\n","ln_output = ln(x)\n","\n","self_attn = MultiHeadedAttention(8, 512)\n","sublayer = lambda x: self_attn(x, x, x, mask)\n","rs = ResidualConnection(512, 0.2)\n","rs_output = rs(x, sublayer)\n","print(\"Residual Layer Output Shape:\", rs_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3veqzXAtXAG","executionInfo":{"status":"ok","timestamp":1705265080150,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"0d826bb0-b4ec-4c00-d916-ade5fae608f0"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Embedding shape: torch.Size([2, 4, 512])\n","Residual Layer Output Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Encoder Layer\n","- putting it all together"],"metadata":{"id":"DiZAUupNuFW3"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        '''\n","        size, dimension of embedding\n","        self_attention, object for multiheaded self attention\n","        feed_forward, layer for feed forward\n","        dropout, rate of dropout\n","        '''\n","        super(EncoderLayer, self).__init__()\n","\n","\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.reslayer = clones(ResidualConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        x = self.reslayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.reslayer[1](x, self. feed_forward)\n","\n","class Encoder(nn.Module):\n","    def __init__(self, layer, n):\n","      '''\n","      layer, what kind of encoder layer\n","      N, number of such layer\n","      '''\n","      super(Encoder, self).__init__()\n","\n","      self.layers = clones(layer, n)\n","      self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n"],"metadata":{"id":"1u1xP6Gdt-Be","executionInfo":{"status":"ok","timestamp":1705266165299,"user_tz":300,"elapsed":1,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["size = 512\n","embedding_dim = 512\n","head_num = 8\n","dim_model = 512\n","dim_ff = 128\n","dropout = 0.2\n","max_len = 60\n","vocab_size = 1000\n","n = 8\n","\n","x = Variable(torch.tensor([[30, 90, 88, 27],\n","                           [403, 254, 337, 20]]))\n","emb = Embeddings(embedding_dim, vocab_size)\n","pe = PositionalEncoding(embedding_dim, dropout, max_len)\n","self_attn = MultiHeadedAttention(head_num, dim_model)\n","ff = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n","\n","# Here we need to deepcopy the layers such that the\n","# the parameters won't be shared\n","c = copy.deepcopy\n","encoder_layer = EncoderLayer(size, c(self_attn), c(ff), dropout)\n","encoder = Encoder(encoder_layer, n)\n","mask = Variable(torch.zeros(8, 4, 4))\n","\n","x = pe(emb(x))\n","y = encoder(x, mask)\n","\n","print(\"Encoder Output Shape:\", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcYAi-Fnve-u","executionInfo":{"status":"ok","timestamp":1705266612061,"user_tz":300,"elapsed":769,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"187bf869-7ecf-4268-e89d-917eb36de0c1"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder Output Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["# Decoder Section\n","- comprised of n decoder\n","- each encoder have three sublayer each withits own residual connection\n","-  1st Sublayer: masked multi-head attention, normalization\n","-  2nd Sublyaer: multi-head attention, normalization\n","-  3rd Sublayer: feedforwar, normalization"],"metadata":{"id":"hPZOgU2w0G65"}},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"BcrEMPVZ7_BI"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        '''\n","        size, emmbedding dimension/encoder output dimension\n","        self_attn, multihead self attention Q = K = V\n","        src_attn, multi head attention where K = V, Q != K or V\n","        feed_forward, fully connected layer\n","        dropout, rate of dropout\n","        '''\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn  = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(ResidualConnection(size, dropout), 3)\n","\n","    def forward(self, x, memory, source_mask, target_mask):\n","        '''\n","        x, input\n","        memory, attention matrix from encoder\n","        source mask\n","        target mask\n","        '''\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, source_mask))\n","        return self.sublayer[2](x, self.feed_forward)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, layer, n):\n","        '''\n","        layer, decoder layer\n","        n, number of layer\n","        '''\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, n)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, memory, source_mask, target_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, source_mask, target_mask)\n","        return self.norm(x)"],"metadata":{"id":"Q2m0G5mMwQUN","executionInfo":{"status":"ok","timestamp":1705268013037,"user_tz":300,"elapsed":1,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["size = 512\n","embedding_dim = 512\n","head_num = 8\n","dim_model = 512\n","dim_ff = 128\n","dropout = 0.2\n","max_len = 60\n","vocab_size = 1000\n","n = 8\n","\n","x = Variable(torch.tensor([[30, 90, 88, 27],\n","                           [403, 254, 337, 20]]))\n","emb = Embeddings(embedding_dim, vocab_size)\n","pe = PositionalEncoding(embedding_dim, dropout, max_len)\n","attn = MultiHeadedAttention(head_num, dim_model)\n","self_attn = src_attn = MultiHeadedAttention(head_num, dim_model, dropout)\n","ff = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n","\n","# Here we need to deepcopy the layers such that the\n","# the parameters won't be shared\n","c = copy.deepcopy\n","encoder_layer = EncoderLayer(size, c(self_attn), c(ff), dropout)\n","encoder = Encoder(encoder_layer, n)\n","mask = Variable(torch.zeros(8, 4, 4))\n","source_mask = target_mask = mask\n","\n","decoder_layer = DecoderLayer(dim_model, c(attn), c(attn), c(ff), dropout)\n","decoder = Decoder(decoder_layer, n)\n","\n","x = pe(emb(x))\n","memory = encoder(x, mask)\n","y = decoder(x, memory, source_mask, target_mask)\n","\n","\n","print(\"Decoder Output Shape:\", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCtW7Za83pgS","executionInfo":{"status":"ok","timestamp":1705268059547,"user_tz":300,"elapsed":979,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"5af41593-a696-42c8-9ecc-5c6085855a2e"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder Output Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Generator"],"metadata":{"id":"xAcXTMex8BeS"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, dim_model, vocab_size):\n","        '''\n","        dim_model, embedding dimenison\n","        vocab_size, size of vocabulary\n","        '''\n","        super(Generator, self).__init__()\n","        self.project = nn.Linear(dim_model, vocab_size)\n","    def forward(self, x):\n","        # Function can be log or normal does not matter as much\n","        return F.log_softmax(self.project(x), dim=-1)"],"metadata":{"id":"bkMB10Nn8C8f","executionInfo":{"status":"ok","timestamp":1705269068609,"user_tz":300,"elapsed":669,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["size = 512\n","embedding_dim = 512\n","head_num = 8\n","dim_model = 512\n","dim_ff = 128\n","dropout = 0.2\n","max_len = 60\n","vocab_size = 1000\n","n = 8\n","\n","x = Variable(torch.tensor([[30, 90, 88, 27],\n","                           [403, 254, 337, 20]]))\n","emb = Embeddings(embedding_dim, vocab_size)\n","pe = PositionalEncoding(embedding_dim, dropout, max_len)\n","attn = MultiHeadedAttention(head_num, dim_model)\n","self_attn = src_attn = MultiHeadedAttention(head_num, dim_model, dropout)\n","ff = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n","\n","# Here we need to deepcopy the layers such that the\n","# the parameters won't be shared\n","c = copy.deepcopy\n","encoder_layer = EncoderLayer(size, c(self_attn), c(ff), dropout)\n","encoder = Encoder(encoder_layer, n)\n","mask = Variable(torch.zeros(8, 4, 4))\n","source_mask = target_mask = mask\n","\n","decoder_layer = DecoderLayer(dim_model, c(attn), c(attn), c(ff), dropout)\n","decoder = Decoder(decoder_layer, n)\n","\n","gen = Generator(dim_model, vocab_size)\n","\n","x = pe(emb(x))\n","memory = encoder(x, mask)\n","y = decoder(x, memory, source_mask, target_mask)\n","\n","print(\"Output Probabilities:\", gen(y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN79eHvK8yrB","executionInfo":{"status":"ok","timestamp":1705269068609,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"12ff1040-2272-4b07-a284-9887a09dc1af"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Output Probabilities: tensor([[[-6.5075, -6.5529, -7.1920,  ..., -7.2614, -6.3057, -6.9275],\n","         [-7.2721, -7.2344, -7.7684,  ..., -6.1604, -6.4811, -7.8577],\n","         [-7.0733, -7.6152, -6.0051,  ..., -7.3485, -7.1835, -7.3827],\n","         [-8.1208, -6.6109, -7.4040,  ..., -6.2986, -7.5493, -7.1977]],\n","\n","        [[-7.0054, -7.6876, -6.9287,  ..., -6.1694, -6.9670, -7.8618],\n","         [-5.9565, -6.3467, -7.0014,  ..., -7.3230, -7.7779, -7.0307],\n","         [-7.0521, -6.5760, -7.1587,  ..., -7.2381, -7.0088, -7.3629],\n","         [-6.6652, -6.4555, -5.7295,  ..., -5.9936, -8.4863, -6.7258]]],\n","       grad_fn=<LogSoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["# Transformer Implemenation"],"metadata":{"id":"Y-bdopTK5ceO"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = source_embed\n","        self.tgt_embed = target_embed\n","        self.generator = generator\n","\n","    def forward(self, source, target, source_mask, target_mask):\n","        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)\n","\n","    def encode(self, source, source_mask):\n","        return self.encoder(self.src_embed(source), source_mask)\n","\n","    def decode(self, memory, source_mask, target, target_mask):\n","        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)"],"metadata":{"id":"s0uFaRSU4zPu","executionInfo":{"status":"ok","timestamp":1705269381344,"user_tz":300,"elapsed":2,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["size = 512\n","embedding_dim = 512\n","head_num = 8\n","dim_model = 512\n","dim_ff = 128\n","dropout = 0.2\n","max_len = 60\n","vocab_size = 1000\n","n = 8\n","\n","x = Variable(torch.tensor([[30, 90, 88, 27],\n","                           [403, 254, 337, 20]]))\n","emb = Embeddings(embedding_dim, vocab_size)\n","pe = PositionalEncoding(embedding_dim, dropout, max_len)\n","attn = MultiHeadedAttention(head_num, dim_model)\n","self_attn = src_attn = MultiHeadedAttention(head_num, dim_model, dropout)\n","ff = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n","\n","# Here we need to deepcopy the layers such that the\n","# the parameters won't be shared\n","c = copy.deepcopy\n","encoder_layer = EncoderLayer(size, c(self_attn), c(ff), dropout)\n","encoder = Encoder(encoder_layer, n)\n","decoder_layer = DecoderLayer(dim_model, c(attn), c(attn), c(ff), dropout)\n","decoder = Decoder(decoder_layer, n)\n","\n","mask = Variable(torch.zeros(8, 4, 4))\n","source_mask = target_mask = mask\n","\n","source_embed = nn.Embedding(vocab_size, dim_model)\n","target_embed = nn.Embedding(vocab_size, dim_model)\n","\n","# for testing's sake we let the source and target/label  and src_mask and tgt_mask\n","# be the same will not happen in real world\n","source = target = x\n","source_mask = target_mask = mask\n","\n","generator = Generator(dim_model,  vocab_size)\n","\n","autoencoder = Transformer(encoder, decoder, source_embed, target_embed, generator)\n","output = autoencoder(source, target, source_mask, target_mask)\n","\n","print(\"Decoder Output Shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmaIB8db6xyS","executionInfo":{"status":"ok","timestamp":1705269381767,"user_tz":300,"elapsed":424,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"6001957a-a127-4968-cf95-e6235ff6d209"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder Output Shape: torch.Size([2, 4, 512])\n"]}]},{"cell_type":"markdown","source":["## Transformer Model for Machine Translation"],"metadata":{"id":"Trwvv5Ez-F5_"}},{"cell_type":"markdown","source":["## Data Preprocessing\n","- cut_zh(), separate chinese sequence into words and adds space between them"],"metadata":{"id":"wpxJ7e8cLrFz"}},{"cell_type":"code","source":["class PrepareData:\n","    def __init__(self, en_path, cn_path, quantity):\n","        # Read in the file, split Chinese data by Word\n","        self.data_zh = self.read_data('chinese.zh')[:quantity]\n","        self.data_en = self.read_data('english.en')[:quantity]\n","        self.data_zh = self.cut_zh(self.data_zh)\n","\n","        # Read into Seqeuence\n","        self.seq_en, self.seq_zh = self.load_data(self.data_en, self.data_zh)\n","\n","        # Build Dictionary\n","        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.seq_en)\n","        self.zh_word_dict, self.zh_total_words, self.zh_index_dict = self.build_dict(self.seq_zh)\n","\n","        # Convert Sequences into ids\n","        self.seq_en_ids, self.seq_zh_ids = self.word_2_id(self.seq_en, self.seq_zh, self.en_word_dict, self.zh_word_dict)\n","\n","        # Pad sequence\n","        self.pad_en, self.pad_zh = self.pad_sequence(self.seq_en_ids, self.seq_zh_ids, 60)\n","\n","        # Batch Sequences\n","        self.batch_en, self.batch_zh = self.split_batch(self.pad_en, self.pad_zh, 32)\n","\n","\n","    def read_data(self, path):\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            data = pd.Series(f.readlines())\n","            data = data.str.replace('\\n', ' ')\n","        return data\n","\n","    def cut_zh(self, data):\n","        data_cut =  data.apply(lambda x: \" \".join(jieba.cut(x)))\n","        return data_cut\n","\n","    def load_data(self, eng, zh):\n","        assert len(zh) == len(eng)\n","        length = len(zh)\n","        en = []\n","        cn = []\n","\n","        for i in range(length):\n","            en.append([\"BOS\"] + word_tokenize(eng[i])+ [\"EOS\"])\n","            cn.append([\"BOS\"] + word_tokenize(zh[i]) + [\"EOS\"])\n","        return en, cn\n","\n","    def build_dict(self, sentences, max_words = 50000):\n","        word_count = Counter()\n","\n","        for sentence in sentences:\n","            for word in sentence:\n","                word_count[word] += 1\n","\n","        # returns from the 1st to the max_words th most common\n","        # words and their number of  appearance\n","        ls = word_count.most_common(max_words)\n","        # total words is counted with BOS and EOS\n","        total_words = len(ls) + 2\n","\n","        word_dict = {w[0]: index + 2 for index, w in enumerate(ls)}\n","        word_dict[\"UNK\"] = len(word_dict.keys())+1\n","        word_dict[\"PAD\"] = len(word_dict.keys())+1\n","\n","        index_dict = {value: key for key, value in word_dict.items()}\n","\n","        return word_dict, total_words, index_dict\n","\n","    def word_2_id(self, en, cn, en_dict, cn_dict, sort=True):\n","        '''\n","        en, eng sequence\n","        cn, chinese sequence\n","        en_dict, eng dict\n","        cn_dict, cn dict\n","        sort, if sort sequence by length\n","        replace each word with their index in dictionary\n","        '''\n","        seq_num = len(en)\n","        seq_en_ids = [[en_dict[token] for token in seq] for seq in en]\n","        seq_cn_ids = [[cn_dict[token] for token in seq] for seq in cn]\n","\n","        def len_sort(seq):\n","            '''\n","            Given sequences, sort the sequence by length\n","            '''\n","            return sorted(range(len(seq)), key = lambda x: len(seq[x]))\n","\n","        if sort:\n","            sorted_index = len_sort(seq_en_ids)\n","            seq_en_ids = [seq_en_ids[i] for i in sorted_index]\n","            seq_cn_ids = [seq_cn_ids[i] for i in sorted_index]\n","\n","        return seq_en_ids, seq_cn_ids\n","\n","    def pad_sequence(self, en, cn, max_len):\n","        token = self.en_word_dict[\"PAD\"]\n","        max_len = min(max([len(seq) for seq in en]), max_len)\n","        out_en = [seq + [token] * (max_len - len(seq)) if max_len >= len(seq) else seq[:max_len] for seq in en]\n","        out_cn = [seq + [token] * (max_len - len(seq)) if max_len >= len(seq) else seq[:max_len] for seq in cn]\n","        return out_en, out_cn\n","\n","    def split_batch(self, en, cn, batch_size, shuffle=True):\n","        out_en = []\n","        out_cn = []\n","\n","        indices = np.arange(0, len(en))\n","        batched = False\n","        while batched == False:\n","          try:\n","              indices = indices.reshape((len(en)//batch_size, batch_size))\n","              batched = True\n","          except ValueError as e:\n","              indices = indices[:-1]\n","\n","        if shuffle: np.random.shuffle(indices)\n","        indices = indices.tolist()\n","\n","        out_en = [[en[index] for index in batch] for batch in indices]\n","        out_cn = [[cn[index] for index in batch] for batch in indices]\n","\n","        return np.array(out_en), np.array(out_cn)\n","\n","    def get_seq(self): return self.seq_en, self.seq_zh\n","    def get_word_dict(self): return self.en_word_dict, self.zh_word_dict\n","    def get_id_dict(self): return self.en_index_dict, self.zh_index_dict\n","    def get_seq_ids(self): return self.seq_en_ids, self.seq_zh_ids\n","    def get_batch(self): return self.batch_en, self.batch_zh\n","    def get_padded(self): return self.pad_en, self.pad_zh\n","\n"],"metadata":{"id":"mEHXhRonQ-BW","executionInfo":{"status":"ok","timestamp":1705282363643,"user_tz":300,"elapsed":363,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":297,"outputs":[]},{"cell_type":"code","source":["data = PrepareData(\"english.en\", \"chinese.zn\", 200)\n","de, dz = data.get_word_dict()\n","ie, iz = data.get_id_dict()\n","pe, pz = data.get_padded()\n","be, bc = data.get_batch()"],"metadata":{"id":"RQ5zaLMdSkI4","executionInfo":{"status":"ok","timestamp":1705282386506,"user_tz":300,"elapsed":1367,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}}},"execution_count":300,"outputs":[]}]}